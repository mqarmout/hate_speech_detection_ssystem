{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./glove.6B/glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fingernails', 'toenails', 'stringy', 'peeling', 'shove']\n"
     ]
    }
   ],
   "source": [
    "print(find_closest_embeddings(\n",
    "    embeddings_dict[\"twig\"] - embeddings_dict[\"branch\"] + embeddings_dict[\"hand\"]\n",
    ")[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in dictionary= 6\n",
      "Dictionary is =  {'language': 1, 'natural': 2, 'leader': 3, 'prime': 4, 'the': 5, 'text': 6}\n",
      "Dense vector for first word is =>  [-5.79900026e-01 -1.10100001e-01 -1.15569997e+00 -2.99059995e-03\n",
      " -2.06129998e-01  4.52890009e-01 -1.66710004e-01 -1.03820002e+00\n",
      " -9.92410004e-01  3.98840010e-01  5.92299998e-01  2.29900002e-01\n",
      "  1.52129996e+00 -1.77640006e-01 -2.97259986e-01 -3.92349988e-01\n",
      " -7.84709990e-01  1.55939996e-01  6.90769970e-01  5.95369995e-01\n",
      " -4.43399996e-01  5.35139978e-01  3.28530014e-01  1.24370003e+00\n",
      "  1.29719996e+00 -1.38779998e+00 -1.09249997e+00 -4.09249991e-01\n",
      " -5.69710016e-01 -3.46560001e-01  3.71630001e+00 -1.04890001e+00\n",
      " -4.67079997e-01 -4.47389990e-01  6.22999994e-03  1.96490008e-02\n",
      " -4.01609987e-01 -6.29130006e-01 -8.25060010e-01  4.55909997e-01\n",
      "  8.26259971e-01  5.70909977e-01  2.11989999e-01  4.68650013e-01\n",
      " -6.00269973e-01  2.99199998e-01  6.79440022e-01  1.42379999e+00\n",
      " -3.21520008e-02 -1.26029998e-01]\n"
     ]
    }
   ],
   "source": [
    "# code for Glove word embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    " \n",
    "x = {'text', 'the', 'leader', 'prime',\n",
    "     'natural', 'language'}\n",
    " \n",
    "# create the dict.\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x)\n",
    " \n",
    "# number of unique words in dict.\n",
    "print(\"Number of unique words in dictionary=\", \n",
    "      len(tokenizer.word_index))\n",
    "print(\"Dictionary is = \", tokenizer.word_index)\n",
    " \n",
    "# download glove and unzip it in Notebook.\n",
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove*.zip\n",
    " \n",
    "# vocab: 'the': 1, mapping of words with\n",
    "# integers in seq. 1,2,3..\n",
    "# embedding: 1->dense vector\n",
    "def embedding_for_vocab(filepath, word_index,\n",
    "                        embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "     \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
    "                                       embedding_dim))\n",
    " \n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix_vocab[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    " \n",
    "    return embedding_matrix_vocab\n",
    " \n",
    " \n",
    "# matrix for vocab: word_index\n",
    "embedding_dim = 50\n",
    "embedding_matrix_vocab = embedding_for_vocab(\n",
    "    './glove.6B/glove.6B.50d.txt', tokenizer.word_index,\n",
    "  embedding_dim)\n",
    " \n",
    "print(\"Dense vector for first word is => \",\n",
    "      embedding_matrix_vocab[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
